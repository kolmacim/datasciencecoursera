mtcars
mtcars
dim(mtcars)
?mtcars
head(mtcar)
head(mtcars)
ggplot(mtcars, aes(x=mpg, y=am)) + geom_boxplot()
library(ggplot2)
ggplot(mtcars, aes(x=mpg, y=am)) + geom_boxplot()
ggplot(mtcars, aes(x=mpg, y=am, fill= cond)) + geom_boxplot()
ggplot(mtcars, aes(x=mpg, y=am, fill= mpg)) + geom_boxplot()
ggplot(mtcars, aes(mpg,am)) + geom_boxplot()
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot()
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot() + geom_jitter()
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot(aes(fill = factor(am))) + geom_jitter()
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot(aes(fill = factor(am))) + geom_jitter() + scale_fill_discrete(name="Experimental\nCondition")
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot(aes(fill = factor(am))) + geom_jitter() + scale_fill_discrete(name="Experimental\nCondition", breaks=c("0", "1"), labels=c("Automatic", "Manual"))
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot(aes(fill = factor(am))) + geom_jitter() + scale_fill_discrete(name="Transmission Type", breaks=c("0", "1"), labels=c("Automatic", "Manual"))
*?mtcars
?mtcars
ggplot(mtcars, aes(factor(am), mpg))
+ geom_boxplot(aes(fill = factor(am)))
+ geom_jitter() + scale_fill_discrete(name="Transmission Type", breaks=c("0", "1"), labels=c("Automatic", "Manual"))
+ xlab("Transmission Type")
+ ylab("Miles/(US) gallon")
ggplot(mtcars, aes(factor(am), mpg)) + geom_boxplot(aes(fill = factor(am))) + geom_jitter() + scale_fill_discrete(name="Transmission Type", breaks=c("0", "1"), labels=c("Automatic", "Manual")) + xlab("Transmission Type") + ylab("Miles/(US) gallon")
head(mtcars)
?mtcars
?filter
library(dplyr)
filter(mtcars, am == "0")
median(filter(mtcars, am == "0"))
median(filter(mtcars, am == "0")$am)
median(filter(mtcars, am == "0")$mpg)
?mtcars
median(filter(mtcars, am == "1")$mpg)
t.test(mpg ~ am, data = mtcars)
fit_simple <- lm(mpg ~ factor(am), data=mtcars)
summary(fit_simple)
model <- lm(mpg ~ . ,data = mtcars)
summary(model)
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
summary(best_model)
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
summary(best_model)
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
str(mtcars)
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
summary(best_model)
analysis_var <- aov(mpg ~ ., data = mtcars)
summary(analysis_var)
analysis_var <- aov(mpg ~ ., data = mtcars)
analysis_var
lm <- lm(mpg ~ cyl + disp + hp + wt + am, data = mtcars)
summary(lm)
analysis_var <- aov(mpg ~ ., data = mtcars)
summary(analysis_var)
lm <- lm(mpg ~ cyl + disp + hp + wt + am, data = mtcars)
summary(lm)
lm <- lm(mpg ~ cyl + hp + wt + am, data = mtcars)
summary(lm)
lm <- lm(mpg ~ cyl + hp + wt + am, data = mtcars)
summary(lm)
par(mfrow = c(2, 2))
plot(lm)
?par
ggplot(lm)
par(mfrow = c(2, 2))
plot(lm)
install.packages('ggfortify')
install.packages('ggfortify')
library(ggfortify)
autoplot(lm, label.size = 3)
lm <- lm(mpg ~ cyl + disp + hp + wt + am, data = mtcars)
summary(lm)
par(mfrow = c(2, 2))
plot(lm)
lm(formula = mpg ~ cyl + hp + wt + am, data = mtcars)
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
str(mtcars)
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
summary(best_model)
lm <- lm(mpg ~ cyl + hp + wt + am, data = mtcars)
summary(lm)
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
init_model <- lm(mpg ~ ., data = mtcars)
best_model <- step(init_model, direction = "both")
summary(best_model)
lm <- lm(formula = mpg ~ cyl + hp + wt + am, data = mtcars)
summary(lm)
lm(formula = mpg ~ cyl + hp + wt + am, data = mtcars)
summary(lm(formula = mpg ~ cyl + hp + wt + am, data = mtcars))
mtcars
install.packages(caret)
install.packages("caret")
library(caret)
summary(spam)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
head(AlzheimerDisease)
head(data(AlzheimerDisease))
?data
AlzheimerDisease
class(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
library(caret)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
install.packages("doParallel")
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
install.packages("doMC")
install.packages("doMC")
library("caret")
install.packages("caret")
require("caret")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
require("caret")
install.packages("pbkrtest")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages(“caret”, dependencies = c(“Depends”, “Suggests”))
install.packages("caret", dependencies = c("Depends", "Suggests"))
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
install.packages("munsell")
install.packages("munsell")
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
library("caret")
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
library(AppliedPredictiveModeling)
library("caret")
install.packages("caret")
install.packages("AppliedPredictiveModeling")
library("caret")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
?concrete
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
# Another way
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
xnames <- colnames(concrete)[1:8]
featurePlot(x=training[, xnames], y=training$CompressiveStrength, plot="pairs")
# No relation between the outcome and other variables
index <- seq_along(1:nrow(training))
ggplot(data=training, aes(x=index, y=CompressiveStrength)) + geom_point() +
theme_bw()
# Step-like pattern -> 4 categories
library(Hmisc)
cutCompressiveStrength <- cut2(training$CompressiveStrength, g=4)
summary(cutCompressiveStrength)
ggplot(data=training, aes(y=index, x=cutCompressiveStrength)) +
geom_boxplot() + geom_jitter(col="blue") + theme_bw()
# Another way
library(plyr)
splitOn <- cut2(training$Age, g=4)
splitOn <- mapvalues(splitOn,
from=levels(factor(splitOn)),
to=c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col=splitOn)
?set.seed
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(Superplasticizer, data=training)
?createDataPartition
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- training[,grep('^IL', x = names(training) )]
preProc <- preProcess(ss, method='pca', thresh=0.9,
outcome=training$diagnosis)
preProc$rotation # 9
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p=3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
IL <- grep("^IL", colnames(training), value=TRUE)
ILpredictors <- predictors[, IL]
df <- data.frame(diagnosis, ILpredictors)
inTrain <- createDataPartition(df$diagnosis, p=3/4)[[1]]
training <- df[inTrain, ]
testing <- df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method="glm", data=training)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
acc1 <- C1$overall[1]
acc1 # Non-PCA Accuracy: 0.65
modelFit <- train(training$diagnosis ~ .,
method="glm",
preProcess="pca",
data=training,
trControl=trainControl(preProcOptions=list(thresh=0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
acc2 <- C2$overall[1]
acc2 # PCA Accuracy: 0.72
head(cars)
new.cars <- data.frame(wt=c(1.7, 2.4, 3.6))
predict(Model, newdata=new.cars)
head(new.cars)
model <- lm(wt, data = new.cars)
cars
weather
set.seed(123)
# randomly pick 70% of the number of observations (365)
index <- sample(1:nrow(weather),size = 0.7*nrow(weather))
data(spam)
library(caret)
library(kernlab)
install.packages("KERNLAB")
install.packages("kernlab")
library(caret)
library(kernlab)
data(spam)
spam
inTrain <- createDataPartition(y = spam$type, p= 0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
head(spam)
spam
spam$type
set.seed(32343)
modelFit <- train(type ~., data = training, method = "glm")
set.seed(32343)
modelFit <- train(type ~., data = training, method = "glm")
modelFit
install.packages("e1071")
set.seed(32343)
modelFit <- train(type ~., data = training, method = "glm")
modelFit
modelFit$finalModel
predictions <- predict(modelFit, newdata = testing)
prediction
predictions
confusionMatrix(predictions, testing$type)
predictions
testing
clear
colnames(spam)
testing[, -58]
colnames(spam)
newtesting <- testing[, -58]
predictions <- predict(modelFit, newdata = newtesting)
confusionMatrix(predictions, newtesting$type)
predictions
confusionMatrix(predictions, newtesting$type)
predictions <- predict(modelFit, newdata = testing)
#testing
#predictions
confusionMatrix(predictions, testing$type)
data(wage)
library(ISLR)
library(ggplot2)
library(caret)
install.packages("ISLR")
data(Wage)
data(wage)
library(ISLR)
data(Wage)
summary(Wage)
Wage
summary(Wage)
nrow(Wage)
colnames(Wage)
inTrain <- createDataPartition(y = Wage$wage, p= 0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x =training[,c("age", "education", "jobclass")], y = training$wage, plot = "pairs")
qplot(age, wage, data=training)
qplot(age, wage, colour = jobclass, data=training)
qq <- qplot(age, wage, colour = education, data=training)
qq + geom_smooth(method ='lm', formula = y ~ x)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE) # 60% training
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
modFit$finalModel
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
install.packages("rpart.plot")
suppressMessages(library(rattle))
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[, -1]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
fitRf <- train(y ~ ., data=vowel.train, method="rf") #random forest
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predRf <- predict(fitRf, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf") #random forest
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predRf <- predict(fitRf, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
fitRf <- train(y ~ ., data=vowel.train, method="rf") #random forest
library(caret)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf") #random forest
fitRf <- train(y ~ ., data=vowel.train, method="rf") #random forest
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predRf <- predict(fitRf, vowel.test)
predGBM <- predict(fitGBM, vowel.test)
confusionMatrix(predRf, vowel.test$y)$overall[1]
confusionMatrix(predGBM, vowel.test$y)$overall[1]
pred <- data.frame(predRf, predGBM, y=vowel.test$y, agree=predRf == predGBM)
head(pred)
accuracy <- sum(predRf[pred$agree] == pred$y[pred$agree]) / sum(pred$agree)
accuracy
library(gbm)
set.seed(62433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData <- data.frame(diagnosis, predictors)
inTrain <- createDataPartition(adData$diagnosis, p=3/4)[[1]]
training <- adData[inTrain, ]
testing <- adData[-inTrain, ]
dim(adData) # 333 131
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
pred <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
# Stack the predictions together using random forests ("rf")
fit <- train(diagnosis ~., data=pred, method="rf")
predFit <- predict(fit, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
#Set the seed to 233 and fit a lasso model to predict Compressive Strength.
#Which variable is the last coefficient to be set to zero as the penalty increases? (Hint: it may be useful to look up ?plot.enet).
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
#Set the seed to 233 and fit a lasso model to predict Compressive Strength.
#Which variable is the last coefficient to be set to zero as the penalty increases? (Hint: it may be useful to look up ?plot.enet).
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit
plot.enet(fit$finalModel, xvar="penalty", use.color=T) # Cement
fit <- bats(tstrain)
library(lubridate)  # For year() function below
#dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate)  # For year() function below
#dat = read.csv("~/Desktop/gaData.csv")
dat = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
fit <- bats(tstrain)
library(lubridate)
library(forecast)
install.packages("forecast")
library(forecast)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) & (predComb$visitsTumblr < predComb$Hi.95)
prop.table(table(predComb$in95))[2] # 0.9617021
install.packages(shiny)
install.packages("shiny")
library(shiny)
setwd("C:/Users/KOLMACIM/Desktop/FTS/COURSERA_DATA_SCIENCE/9_Data_Products/2_Week")
library(devtools)
#require(devtools)
#install_github("slidify", "ramnathv")
#install_github("slidifyLibraries", "ramnathv")
library(slidify)
author("first_deck")
author('test')
slidify("index.Rmd")
library(knitr)
browseURL("index.rmd")
browseURL("index.html")
